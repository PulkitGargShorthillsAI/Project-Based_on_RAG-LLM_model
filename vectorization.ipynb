{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading environment variables...\n",
      "âœ… Pinecone API Key loaded successfully!\n",
      "ðŸ” Checking if Pinecone index 'chatbot' exists...\n",
      "ðŸ†• Creating a new Pinecone index: chatbot\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”„ Loading environment variables...\")\n",
    "GEMINI_API = os.getenv(\"GEMINI_API\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"âŒ PINECONE_API_KEY is not set. Please check your environment variables.\")\n",
    "\n",
    "\n",
    "print(\"âœ… Pinecone API Key loaded successfully!\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "index_name = \"chatbot\"\n",
    "\n",
    "print(f\"ðŸ” Checking if Pinecone index '{index_name}' exists...\")\n",
    "\n",
    "if len(pc.list_indexes()):\n",
    "    flag = True\n",
    "    for index in pc.list_indexes():\n",
    "        if index_name == index['name']:\n",
    "            print(\"Index already exists\")\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        print(f\"ðŸ†• Creating a new Pinecone index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=768,\n",
    "            metric=\"cosine\",  # Replace with your model metric\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    print(f\"ðŸ†• Creating a new Pinecone index: {index_name}\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",  # Replace with your model metric\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading text files from: /home/shtlp_0101/Documents/Project-Based_on_RAG-LLM_model/scraped_city_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load text files\n",
    "folder_path = \"/home/shtlp_0101/Documents/Project-Based_on_RAG-LLM_model/scraped_city_data\"\n",
    "print(f\"ðŸ“‚ Loading text files from: {folder_path}\")\n",
    "text_documents = []\n",
    "\n",
    "\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    " \n",
    " \n",
    "#Extract Data From the PDF File\n",
    "def load_txt_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.txt\",\n",
    "                            loader_cls=TextLoader)\n",
    " \n",
    "    documents=loader.load()\n",
    " \n",
    "    return documents\n",
    " \n",
    " \n",
    "extracted_data = load_txt_file(folder_path)\n",
    " \n",
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    " \n",
    " \n",
    "text_chunks = text_split(extracted_data)\n",
    "#Download the Embeddings from HuggingFace\n",
    "def embeddings():\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API)\n",
    "    return embeddings\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Uploading documents to Pinecone...\n",
      "âœ… Documents stored in Pinecone!\n",
      "ðŸ” Retriever initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY  # Explicitly set the API key\n",
    "# Correcting the embeddings initialization\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API\"))\n",
    "\n",
    "# Upload documents to Pinecone\n",
    "print(\"ðŸ“¤ Uploading documents to Pinecone...\")\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,  \n",
    "    index_name=index_name,\n",
    "    embedding=embeddings  # No parentheses here\n",
    ")\n",
    "print(\"âœ… Documents stored in Pinecone!\")\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "print(\"ðŸ” Retriever initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API = os.getenv(\"OPENAI_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAI_API' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,api_key\u001b[38;5;241m=\u001b[39m\u001b[43mOPENAI_API\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OPENAI_API' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500,api_key=OPENAI_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=GEMINI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York is known for having a restaurant for every cuisine, with Italian-American and Jewish influences being the biggest. You can find terrific pizza and unforgettable smoked-salmon bagels at the delis. Katz Diner is also a famous place, known for its pastrami.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"famous places for food in nyc?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandni Chowk is the bustling heart of Old Delhi, known for weaving together centuries of history, culture, and commerce. It is a vibrant market with narrow lanes and chaotic energy. It is a testament to Indiaâ€™s rich past and its dynamic present.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Speciality of chandni chowk?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaipur offers kathi rolls, kachoris, and kulfis, with Rawat Sweets being famous for pyaz kachoris. Agra is known for its wide variety of petha, street-side biryani, and kebabs. Manali has Mall Road's chaats, samosas, and ice cream, while Mysore is known for Mysore Pak, dosas, and Gobi Manchurian.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what are the best food items there?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
