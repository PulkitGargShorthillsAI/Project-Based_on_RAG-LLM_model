{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading environment variables...\n",
      "‚úÖ Pinecone API Key loaded successfully!\n",
      "üîç Checking if Pinecone index 'chatbot' exists...\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Loading environment variables...\")\n",
    "GEMINI_API = os.getenv(\"GEMINI_API\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"‚ùå PINECONE_API_KEY is not set. Please check your environment variables.\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pinecone API Key loaded successfully!\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "index_name = \"chatbot\"\n",
    "\n",
    "print(f\"üîç Checking if Pinecone index '{index_name}' exists...\")\n",
    "\n",
    "for index in pc.list_indexes():\n",
    "    if index_name == index['name']:\n",
    "        print(\"Index already exists\")\n",
    "    else:\n",
    "        print(f\"üÜï Creating a new Pinecone index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=768,\n",
    "            metric=\"cosine\",  # Replace with your model metric\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading text files from: /home/shtlp_0101/Documents/Project-Based_on_RAG-LLM_model/scraped_city_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load text files\n",
    "folder_path = \"/home/shtlp_0101/Documents/Project-Based_on_RAG-LLM_model/scraped_city_data\"\n",
    "print(f\"üìÇ Loading text files from: {folder_path}\")\n",
    "text_documents = []\n",
    "\n",
    "\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    " \n",
    " \n",
    "#Extract Data From the PDF File\n",
    "def load_txt_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.txt\",\n",
    "                            loader_cls=TextLoader)\n",
    " \n",
    "    documents=loader.load()\n",
    " \n",
    "    return documents\n",
    " \n",
    " \n",
    "extracted_data = load_txt_file(folder_path)\n",
    " \n",
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    " \n",
    " \n",
    "text_chunks = text_split(extracted_data)\n",
    "#Download the Embeddings from HuggingFace\n",
    "def embeddings():\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API)\n",
    "    return embeddings\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading documents to Pinecone...\n",
      "‚úÖ Documents stored in Pinecone!\n",
      "üîç Retriever initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY  # Explicitly set the API key\n",
    "# Correcting the embeddings initialization\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API\"))\n",
    "\n",
    "# Upload documents to Pinecone\n",
    "print(\"üì§ Uploading documents to Pinecone...\")\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,  \n",
    "    index_name=index_name,\n",
    "    embedding=embeddings  # No parentheses here\n",
    ")\n",
    "print(\"‚úÖ Documents stored in Pinecone!\")\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "print(\"üîç Retriever initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API = os.getenv(\"OPENAI_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0.4, max_tokens=500,api_key=OPENAI_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=GEMINI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi is known for its historical aura and vibrant markets. Some famous places to visit include Humayun‚Äôs Tomb, Qutub Minar, Red Fort, and Akshardham Temple. The Red Fort, also known as Lal Qila, is a UNESCO World Heritage Site.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"famous places in delhi?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandni Chowk is the bustling heart of Old Delhi, known for weaving together centuries of history, culture, and commerce. It is a vibrant market with narrow lanes and chaotic energy. It is a testament to India‚Äôs rich past and its dynamic present.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Speciality of chandni chowk?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaipur offers kathi rolls, kachoris, and kulfis, with Rawat Sweets being famous for pyaz kachoris. Agra is known for its wide variety of petha, street-side biryani, and kebabs. Manali has Mall Road's chaats, samosas, and ice cream, while Mysore is known for Mysore Pak, dosas, and Gobi Manchurian.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what are the best food items there?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
