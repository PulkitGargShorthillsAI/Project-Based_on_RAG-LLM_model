{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shtlp_0101/Documents/Project-Based_on_RAG-LLM_model/venv/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import PromptTemplate\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv('GEMINI_API2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class QA(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    expected_context:str\n",
    "    question_type:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('GEMINI_API2'))\n",
    "\n",
    "# Function to generate Q&A pairs using Gemini Pro\n",
    "def generate_qa(text):\n",
    "    prompt = f'''You are an expert dataset creator tasked with generating a high-quality golden dataset for evaluating a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "Given the following document, generate a diverse set of questions along with their ground truth answers and the expected retrieval context (i.e., the passage or section of the document where the answer is found). Ensure that the questions cover multiple dimensions of complexity and robustness, including:\n",
    "\n",
    "### Types of Questions:\n",
    "1. **Fact-based questions** (Simple, direct queries such as Who, What, When, Where)\n",
    "2. **Comparative questions** (Questions comparing entities or concepts, e.g., How is X different from Y?)\n",
    "3. **Causal questions** (Questions exploring cause-effect relationships, e.g., Why did X happen?)\n",
    "4. **Complex/multi-hop reasoning questions** (Questions requiring synthesis or multiple steps of reasoning)\n",
    "5. **Edge cases and adversarial questions:**  \n",
    "   - Questions that are ambiguous or may be interpreted in multiple ways  \n",
    "   - Out-of-context or unanswerable questions  \n",
    "   - Gibberish or illogical questions\n",
    "\n",
    "For each question, generate a precise answer strictly based on the document and identify the exact passage (expected retrieval context) that supports the answer. For questions that are not answerable based on the document (e.g., out-of-context or gibberish queries), set the answer to:\n",
    "\"This question cannot be answered based on the given document.\" and leave the expected context field empty.\n",
    "\n",
    "**Document:**  \n",
    "{text}\n",
    "\n",
    "### Instructions:\n",
    "- Generate at least 5-10 questions per document, ensuring a balance across the categories mentioned above.\n",
    "- Label each question with its type: \"Fact-based\", \"Comparative\", \"Causal\", \"Complex\", or \"Edge Case\".\n",
    "- Provide a clear and concise ground truth answer that is strictly derived from the provided document.\n",
    "- Extract and include the specific passage (or section) from the document that best supports the answer in the \"expected_context\" field.\n",
    "- For invalid or unanswerable questions, set the answer to \"This question cannot be answered based on the given document.\" and leave \"expected_context\" empty.\n",
    "\n",
    "### Output Format (JSON):\n",
    "```json\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"<Generated Question>\",\n",
    "            \"answer\": \"<Generated Answer>\",\n",
    "            \"question_type\": \"<Fact-based | Comparative | Causal | Complex | Edge Case>\",\n",
    "            \"expected_context\": \"<Passage from the document where the answer is found>\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "'''\n",
    "    response = client.models.generate_content(model=\"gemini-2.0-flash\",contents=prompt,config={'response_mime_type': 'application/json','response_schema': list[QA]})\n",
    "    try:\n",
    "        qa_pairs = json.loads(response.text)\n",
    "        return qa_pairs if isinstance(qa_pairs, list) else []\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = \"qa_output2.json\"\n",
    "output_csv = \"qa_output2.csv\"\n",
    "folder_paths=[\"data2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"processing.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    \"\"\"Load the set of processed files from a progress file.\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_progress(progress_file, processed_files):\n",
    "    \"\"\"Save the set of processed files to a progress file.\"\"\"\n",
    "    with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(processed_files), f, indent=4)\n",
    "    logging.info(\"Progress saved: %d files processed.\", len(processed_files))\n",
    "\n",
    "def save_qa_data(output_json, output_csv, all_qa):\n",
    "    \"\"\"Save the Q&A data to JSON and CSV files.\"\"\"\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_qa, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(output_csv, \"w\", encoding=\"utf-8\", newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Question\", \"Answer\"])\n",
    "        for qa in all_qa:\n",
    "            writer.writerow([qa[\"question\"], qa[\"answer\"]])\n",
    "    \n",
    "    logging.info(\"Q&A data saved: %d pairs generated.\", len(all_qa))\n",
    "\n",
    "def process_folders(folder_paths, output_json, output_csv, progress_file=\"progress.json\"):\n",
    "    \"\"\"Process files in given folders and generate Q&A pairs with checkpointing.\"\"\"\n",
    "    \n",
    "    all_qa = []\n",
    "    processed_files = load_progress(progress_file)  # Load previously processed files\n",
    "    count = 0\n",
    "    \n",
    "    logging.info(\"Starting processing for folders: %s\", \", \".join(folder_paths))\n",
    "    \n",
    "    for folder_path in folder_paths:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Skip if file was processed before or is not a valid document\n",
    "            if filename in processed_files or not os.path.isfile(file_path) or not filename.endswith((\".txt\", \".pdf\")):\n",
    "                continue  \n",
    "\n",
    "            logging.info(\"Processing file: %s from %s\", filename, folder_path)\n",
    "            print(f\"Processing: {filename} from {folder_path}\")\n",
    "            \n",
    "            try:\n",
    "                text = read_file(file_path)\n",
    "                qa_pairs = generate_qa(text)\n",
    "                all_qa.extend(qa_pairs)\n",
    "                processed_files.add(filename)  # Mark file as processed\n",
    "                count += 1\n",
    "                \n",
    "                logging.info(\"Generated %d Q&A pairs for %s\", len(qa_pairs), filename)\n",
    "                print(f\"Generated {len(qa_pairs)} Q&A pairs\")\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error processing %s: %s\", filename, str(e))\n",
    "                continue\n",
    "\n",
    "            # Save progress every 10 files\n",
    "            if count % 14 == 0:\n",
    "                save_qa_data(output_json, output_csv, all_qa)\n",
    "                save_progress(progress_file, processed_files)\n",
    "                logging.info(\"Checkpoint reached: %d files processed. Waiting to avoid timeout.\", count)\n",
    "                print(f\"Checkpoint saved: {count} files processed. Waiting to avoid timeout.\")\n",
    "                time.sleep(30)  # Delay to prevent API timeouts\n",
    "    \n",
    "    # Final save after all processing\n",
    "    save_qa_data(output_json, output_csv, all_qa)\n",
    "    save_progress(progress_file, processed_files)\n",
    "    logging.info(\"Final save complete. Processed %d new files.\", count)\n",
    "    print(f\"Final save complete. Processed {count} new files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: patna.txt from data2\n",
      "Generated 26 Q&A pairs\n",
      "Processing: one-places-from-each-state.txt from data2\n",
      "Generated 63 Q&A pairs\n",
      "Processing: mechuka.txt from data2\n",
      "Generated 15 Q&A pairs\n",
      "Processing: imphal.txt from data2\n",
      "Generated 32 Q&A pairs\n",
      "Processing: monuments-of-india.txt from data2\n",
      "Generated 30 Q&A pairs\n",
      "Processing: nohsngithiang-falls.txt from data2\n",
      "Generated 12 Q&A pairs\n",
      "Processing: kochi.txt from data2\n",
      "Generated 21 Q&A pairs\n",
      "Processing: kamakhya-temple.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: netarhat-dam.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: indore.txt from data2\n",
      "Generated 15 Q&A pairs\n",
      "Processing: mahabodhi-temple.txt from data2\n",
      "Generated 7 Q&A pairs\n",
      "Processing: rameswaram.txt from data2\n",
      "Generated 11 Q&A pairs\n",
      "Processing: kangla-fort.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: haridwar.txt from data2\n",
      "Generated 19 Q&A pairs\n",
      "Checkpoint saved: 14 files processed. Waiting to avoid timeout.\n",
      "Processing: leh.txt from data2\n",
      "Generated 35 Q&A pairs\n",
      "Processing: mumbai.txt from data2\n",
      "Generated 19 Q&A pairs\n",
      "Processing: hyderabad.txt from data2\n",
      "Generated 23 Q&A pairs\n",
      "Processing: hampi.txt from data2\n",
      "Generated 68 Q&A pairs\n",
      "Processing: puducherry.txt from data2\n",
      "Generated 27 Q&A pairs\n",
      "Processing: manali.txt from data2\n",
      "Generated 14 Q&A pairs\n",
      "Processing: ranchi.txt from data2\n",
      "Generated 18 Q&A pairs\n",
      "Processing: naida-caves.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: har-ki-pauri.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: khawnglung-wildlife-sanctuary.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: kavaratti.txt from data2\n",
      "Generated 19 Q&A pairs\n",
      "Processing: raipur.txt from data2\n",
      "Generated 14 Q&A pairs\n",
      "Processing: hawa-mahal.txt from data2\n",
      "Generated 10 Q&A pairs\n",
      "Processing: international_locations.txt from data2\n",
      "Generated 48 Q&A pairs\n",
      "Checkpoint saved: 28 files processed. Waiting to avoid timeout.\n",
      "Final save complete. Processed 28 new files.\n"
     ]
    }
   ],
   "source": [
    "process_folders(folder_paths, output_json, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
